# -*- coding: utf-8 -*-
"""Trabalho2deCD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19wVmIBRM8x4NayvYtLIyVob4rFqmWVkl
"""

import pandas as pd

df = pd.read_csv("saida.csv")

variancia = df.loc[:, 'ano'].var(ddof=0) #ddof é igual a 0 para poder pegar a variancia populacional e não amostral
print("O valor da variância  = " + str(variancia))

desviopad = df.loc[:, 'ano'].std(ddof=0) #calculando desvio padrão
print("O valor do desvio padrao  = " + str(desviopad))

media = df.loc[:, 'ano'].mean() #calculando média 
print("O valor da media  = " + str(media))

mediana = df.loc[:, 'ano'].median() #calculando mediana 
print("O valor da mediana  = " + str(mediana))


min = df.loc[:, 'ano'].min() #qual o menor ano?
print("O valor do menor ano  = " + str(min))

max = df.loc[:, 'ano'].max() #qual o maior ano?
print("O valor do maior ano  = " + str(max))

moda = df.loc[:,'ano'].mode()[0] #calculando moda
print("O valor da moda  = " + str(moda))

series = df["ano"].value_counts() #armazena cada valor único da coluna e também quantas essse valor aparece
series2 = df["tipo"].value_counts() #armazena cada valor único da coluna e também quantas essse valor aparece

total2 = series2.sum() #fazendo somatório 
#print(total2)

df33 = pd.DataFrame(series2) #criando uma tabela com o valor de series
#print(df33)

df33['frequencia %'] = df33['tipo']*100 / total2

#print(df33)
tabelaTipo = pd.DataFrame({ "Num. Freq": df33['tipo']}) #criando a tabela de frequência

tabelaTipo.sort_index(inplace=True) #ordenando a tabela

#tabela de frequência com base nos gêneros musicais
print(tabelaTipo)

total = series.sum() #fazendo somatório dos números
#print(total)

df1 = pd.DataFrame(series) #criando uma tabela com o valor de series
#print(df1)

df1['frequencia %'] = df1['ano']*100 / total

print(df1)

tabela = pd.DataFrame({ "Num. Freq": df1['ano'], "Freq. %": df1['frequencia %']}) #criando a tabela de frequência

tabela.sort_index(inplace=True) #ordenando a tabela

#tabela de frequência com base nos anos 
print(tabela)

tabela.to_csv("Resultado.csv") #salvando a tabela de frequência em um arquivo csv

df.boxplot(column='ano') #criando o boxplot usando os dados dos anos

#não temos outlier
#O valor máximo é 1999 (ou seja antes de 2000)
#O 3º Quartil está entre 1980 e 1990
#O 2º Quartil está entre 1960 e 1970
#O 1º Quartil está entre 1940 e 1955
#O valor mínimo é 1900

#Teste de normalidade
import scipy.stats as stats
shapiro_stat, shapiro_p_valor = stats.shapiro(df['ano']) #Testede normalidade de shapiro-wilk
print("O valor estatistico de shapiro-Wilk = " + str(shapiro_stat))
print("O valor p de shapiro-Wilk = " + str(shapiro_p_valor))

#conclusão do teste
if(shapiro_p_valor > 0.05):
  print("Os dados são similares a uma distribuicao normal")
else:
  print("Os dados não são similares a uma distribuicao normal")

df['ano'].plot.hist() #criando um grafico de histograma com os anos 

#Como já é visível os anos estão dispostos em ordem crescente
#Existem mais músicas concetradas entre os anos 1990 à 1999
#Existem menos músicas concetradas entre os anos 1900 à 1910

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

df = df[df['ano'].isnull() == False]

counts = df['ano'].value_counts()

counts.index = counts.index.map(str)

wordcloud = WordCloud().generate_from_frequencies(counts)
plt.figure()
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()

import seaborn as sn
dadoFreq = pd.read_csv("Resultado.csv")
correlation = dadoFreq.corr()
plot = sn.heatmap(correlation, annot = True, fmt=".1f", linewidths=.6)
plot

!pip install researchpy
import researchpy #com este pacote podemos obter o V de Cramer 

#(O coeficiente V de Cramer serve para medir associação em tabelas não quadradas.)

contTable = pd.crosstab(df['tipo'], df['ano'])
print(contTable)

crosstab, res = researchpy.crosstab(df['tipo'], df['ano'], test='chi-square')
print(res)

# Com isso sabemos que o grau de Cramer é 0.4918, ou seja, 0=<v=>1. 

#Coeficiente de contingencia V de Cramer. Toma valores entre 0 e 1. O valor 0
#corresponde a ausencia de associacao entre as variaveis, valores proximos de zero
#correspondem a fraca associacao e valores mais proximos de 1 correspondem a
#associacao mais forte.

#Com grau de cramer sendo 0.4918 então sabemos que possui fraca associação

# instalar wordcloud
!pip install wordcloud -q

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

summary = df.dropna(subset=['tipo'], axis=0)['tipo']
#eliminar as colunas com valores ausentes

all_summary = " ".join(s for s in summary)
#Eliminei os valores nulos da coluna tipo e atribui a Series a outra variável. Para criar a wordcloud, vou concatenar todas as strings, como se fosse uma linha única.

# lista de stopword
stopwords = set(STOPWORDS)

# gerar uma wordcloud
wordcloud = WordCloud(stopwords=stopwords,
                      background_color="black").generate(all_summary)

# mostrar a imagem final
fig, ax = plt.subplots(figsize=(10,6))
ax.imshow(wordcloud, interpolation='bilinear')

plt.tight_layout()